{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"在训练参数化机器学习模型时，权重衰减（weight decay）是最⼴泛使⽤的正则化的技术之⼀，它通常也被称为L2正则化。这项技术通过函数与零的距离来衡量函数的复杂度，因为在所有函数f中，函数f = 0（所有输⼊都得到值0）  ","metadata":{}},{"cell_type":"code","source":"!pip install d2l\nimport torch\nfrom torch import nn\nfrom d2l import torch as d2l","metadata":{"execution":{"iopub.status.busy":"2022-06-03T02:49:37.108687Z","iopub.execute_input":"2022-06-03T02:49:37.109577Z","iopub.status.idle":"2022-06-03T02:49:46.208136Z","shell.execute_reply.started":"2022-06-03T02:49:37.109538Z","shell.execute_reply":"2022-06-03T02:49:46.207277Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\ntrue_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\ntrain_data = d2l.synthetic_data(true_w, true_b, n_train)\ntrain_iter = d2l.load_array(train_data, batch_size)\ntest_data = d2l.synthetic_data(true_w, true_b, n_test)\ntest_iter = d2l.load_array(test_data, batch_size, is_train=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T02:49:46.210009Z","iopub.execute_input":"2022-06-03T02:49:46.210360Z","iopub.status.idle":"2022-06-03T02:49:46.232617Z","shell.execute_reply.started":"2022-06-03T02:49:46.210326Z","shell.execute_reply":"2022-06-03T02:49:46.231872Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def train_concise(wd):\n    net = nn.Sequential(nn.Linear(num_inputs,1))\n    # 使用正太分布初始化权重\n    for param in net.parameters():\n        param.data.normal_()\n    # 直接使用损失值\n    loss = nn.MSELoss(reduction='none')\n    num_epochs,lr = 100, 0.003\n    \n    trainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay':wd},\n        {\"params\":net[0].bias}],lr = lr)\n    animator = d2l.Animator(xlabel='epochs',ylabel='loss',yscale='log',\n                           xlim=[5,num_epochs],legend=['train','test'])\n    for epoch in range(num_epochs):\n        for X,y in train_iter:\n            trainer.zero_grad()\n            l = loss(net(X),y)\n            l.mean().backward()\n            trainer.step()\n        if (epoch + 1) % 5 == 0:\n            animator.add(epoch + 1,\n                        (d2l.evaluate_loss(net, train_iter, loss),\n                         d2l.evaluate_loss(net, test_iter, loss)))\n        print('w的L2范数: ', net[0].weight.norm().item())","metadata":{"execution":{"iopub.status.busy":"2022-06-03T02:50:38.573624Z","iopub.execute_input":"2022-06-03T02:50:38.574874Z","iopub.status.idle":"2022-06-03T02:50:38.584260Z","shell.execute_reply.started":"2022-06-03T02:50:38.574834Z","shell.execute_reply":"2022-06-03T02:50:38.583163Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_concise(0)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T02:50:40.429049Z","iopub.execute_input":"2022-06-03T02:50:40.429522Z","iopub.status.idle":"2022-06-03T02:50:45.974201Z","shell.execute_reply.started":"2022-06-03T02:50:40.429483Z","shell.execute_reply":"2022-06-03T02:50:45.973249Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_concise(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T02:50:59.524353Z","iopub.execute_input":"2022-06-03T02:50:59.524808Z","iopub.status.idle":"2022-06-03T02:51:08.405514Z","shell.execute_reply.started":"2022-06-03T02:50:59.524773Z","shell.execute_reply":"2022-06-03T02:51:08.404516Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"暂退法在前向传播过程中，计算每⼀内部层的同时注⼊噪声，这已经成为训练神经⽹络的常⽤技术。这种⽅法之所以被称为暂退法，因为我们从表⾯上看是在训练过程中丢弃（dropout）⼀些神经元。在整个训练过程的每⼀次迭代中，标准暂退法包括在计算下⼀层之前将当前层中的⼀些节点置零。","metadata":{}},{"cell_type":"code","source":"net = nn.Sequential(nn.Flatten(),\n                    nn.Linear(784,256),\n                    nn.ReLU(),\n                    nn.Dropout(0.2),\n                    nn.Linear(256,256),\n                    nn.ReLU(),\n                    nn.Dropout(0.5),\n                    nn.Linear(256,10))\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight,std=0.01)\nnet.apply(init_weights)\nnum_epochs, lr, batch_size = 10, 0.5, 256\nloss = nn.CrossEntropyLoss(reduction='none')\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\ntrainer = torch.optim.SGD(net.parameters(), lr=lr)\nd2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T03:13:38.363428Z","iopub.execute_input":"2022-06-03T03:13:38.364393Z","iopub.status.idle":"2022-06-03T03:14:28.043337Z","shell.execute_reply.started":"2022-06-03T03:13:38.364343Z","shell.execute_reply":"2022-06-03T03:14:28.042327Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"解决（或⾄少减轻）梯度消失和梯度爆炸的⼀种⽅法是进⾏参数初始化，优化期间的注意和适当的正则化也可以进⼀步提⾼稳定性。\n- 默认初始化\n- Xavier初始化 Xavier初始化从均值为零，⽅差$\\sigma^2=\\frac{2}{n_{in}+n_{out}} $ 的⾼斯分布中采样权重","metadata":{}},{"cell_type":"markdown","source":"分布偏移\n- 协变量偏移 虽然输⼊的分布可能随时间⽽改变，但标签函数（即条件分布$P(y|x) $没有改变\n- 标签偏移 标签边缘概率$P(x|y) $可以改变，但是类别条件分布P(x|y)在不同的领域之间保持不变\n- 概念偏移 标签的定义发⽣变化","metadata":{}}]}