{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tanh导数小于1，梯度消失问题，改用relu可缓解  \n",
    "奇异值表示数据离散程度，根据其是否大于1可以预测梯度大小变化   \n",
    "梯度裁剪： $if ||g||\\ge threshold: g = \\frac{threshold}{||g||} g$ 防止梯度爆炸\n",
    "防止梯度消失：LSTM\n",
    "- 输出门 $o=\\sigma(x_tW_x^{(o)}+h_{t-1}W_h^{(o)}+b^{(o)}) \\ h_t=o\\odot tanh(c_t)$ \n",
    "- 遗忘门 $f=\\sigma(x_tW_x^{(f)}+h_{t-1}W_h^{(f)}+b^{(f)}) $\n",
    "- 加入新记忆  $g=tanh(x_tW_x^{(g)}+h_{t-1}W_h^{(g)}+b^{(g)}) \\ c_t=f\\odot c_{t-1}+g$\n",
    "- 记忆门 $i=\\sigma(x_tW_x^{(i)}+h_{t-1}W_h^{(i)}+b^{(i)}) $  \n",
    "- c的产生依赖+和x两种运算，+不改变上游梯度，x来自于遗忘门，可以期待其记住需要长期记忆的信息\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
