{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于计数方法的问题\n",
    "- 矩阵庞大，SVD缓慢  \n",
    "- 添加新词需要重新计算\n",
    "基于推理的方法\n",
    "- 给定上下文，预测单词\n",
    "- 可以根据上一次的权重增量学习  \n",
    "- 就单词相似性的定量评价而言，推理和技术方法难分上下\n",
    "- 使用skip-gram被证明和修改过的共现矩阵有相同作用  \n",
    "word2vec\n",
    "- CBOW $L=-\\frac{1}{T}\\sum^T_{t=1}{\\log P(w_t|w_{t-1},w_{t+1})}$\n",
    "- skip-gram 使用W_in $L=-(\\frac{1}{T}\\sum^T_{t=1}{\\log P(w_{t-1}|w_{t})+\\log P(w_{t+1}|w_{t})})$  任务更难，效果更好\n",
    "glove\n",
    "- 使用W_in和W_out 的和  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from common.util import preprocess,create_contexts_target,convert_one_hot\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self,vocab_size,hidden_size):\n",
    "        V,H = vocab_size, hidden_size\n",
    "\n",
    "        W_in = np.random.randn(V,H).astype('f')\n",
    "        W_out = np.random.randn(H,V).astype('f')\n",
    "\n",
    "\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [] ,[]\n",
    "\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self,contexts,t):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = 0.5 * (h0 + h1)\n",
    "        s = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(s,t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self,dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus,word_to_id,id_to_word=preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "contexts , target = create_contexts_target(corpus,window_size)\n",
    "target = convert_one_hot(target,vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "model = SimpleCBOW(vocab_size,hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model,optimizer)\n",
    "trainer.fit(contexts,target,max_epoch,batch_size)\n",
    "trainer.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word,word_vecs[word_id])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bbcfab25aca44efc94327c358633b06763d7d1ff3b6eda80956186e9f6f95138"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
